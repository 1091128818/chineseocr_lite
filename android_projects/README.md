## 以下内容仅代表benjaminwan个人观点 仅供参考

#### android平台onnxruntime vs ncnn对比测试介绍
1. ChOcrLiteAndroidOnnx项目使用onnxruntime框架，ChOcrLiteAndroidOnnxToNcnn项目使用ncnn框架，用于对比的项目JNI部分均使用C++编写，JVM部分使用Kotlin编写。
2. 除了调用推理框架的代码，其它代码尽量保持一致。
3. 推理框架均不使用GPU或NPU加速，仅使用CPU进行计算。
4. 模型使用chineseocr-lite的onnx分支的模型，ncnn框架是用专用的模型格式，故需要从onnx格式转为ncnn专用格式，理论上是一样的模型，但转换可能造成精度损失。
5. ncnn官方提供了android平台的预编译库，onnxruntime则需要自己编译。
6. 对比测试的项目有：推理速度、编译后apk文件大小

#### 推理速度对比
1. 在同一手机以及同一个PC机(模拟器)上进行测试，对比arm64-v8a以及x86_64这2种ABI。
2. onnxruntime版本为最新HEAD20201020，ncnn版本tag 20200916
3. 测试条件：不计算模型加载、框架初始化、加载图片的时间，以同一张图片，同样的参数，跑100次计算平均耗时。
4. 测试手机，华为荣耀play1，麒麟970，4*Cortex-A73+4*Cortex-A53
5. 测试PC，Intel i5-9400F，6核心6线程，macOS 10.15.7，google模拟器 android 9.0 x86_64，模拟器最多只能配置3个核心
6. 模拟器是3个核心，真机是4个大核+4个小核

| ABI    | 线程数  | onnx耗时 | ncnn耗时 | 耗时倍率 |
| ------- | ------- | ----: | ----: | ----: |
| arm64-v8a | 1  | 1491.78ms | 1552.89ms | 0.96 |
| x86_64    | 1  | 488.96ms | 305.83ms | 1.60 |
| arm64-v8a | 4  | 903.21ms | 829.69ms | 1.09 |
| x86_64    | 4  | 471.77ms | 274.52ms | 1.72 |
| arm64-v8a | 6  | 1088.54ms | 911.35ms | 1.19 |
| x86_64    | 6  | 1276.66ms | 308.60ms | 4.14 |

#### 推理速度对比总结：
1. 1个线程的情况下，arm平台两者速度相当，x86平台ncnn更快。
2. 4个线程的情况下，仍然是arm平台两者速度相当，x86平台ncnn更快。
3. 6个线程的情况下，arm平台ncnn稍快一些，x86平台ncnn快很多。
4. 纵向对比，4个线程时速度最佳，速度最快的是x86_64的4线程，但与x86_64的1线程没有拉开差距。
5. 总体来说，手机端ncnn推理速度更快。

#### 文件大小对比
1. onnxruntime为自己编译的动态库，ncnn为静态库。
2. onnxruntime的大小还能通过自己编译mini_build选项进一步优化，但因为模型格式也得转为ort专用格式，故这里就没有加入对比。
3. 直接对比编译后apk大小。

| 平台     | onnx大小 | ncnn大小 | Size倍率 |
| ------- | -------- | ------: | ------: |
| android |  69.66M  |  60.35M |   1.15倍 |


#### 文件大小对比总结：
1. ncnn在android平台文件大小项目完胜onnx。
2. 没有对比onnx静态编译的情况，所以此对比不够严谨。

#### 总体总结：
1. ncnn为移动端优化，模型大小和文件大小都更小，适合移动端部署。
2. ncnn在移动端的推理速度更快。